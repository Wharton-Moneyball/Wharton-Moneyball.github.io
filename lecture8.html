<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Lecture 8: Logistic Regression</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">WMb'22</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-institution"></span>
     
    Academy
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lecture0.html">Lecture 0</a>
    </li>
    <li class="dropdown-header">Problem Set 0</li>
    <li>
      <a href="tc_lecture0.html">Lecture 0</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    <span class="fa fa-dribbble"></span>
     
    Training Camp
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="tc_lecture0.html">Lecture 0</a>
    </li>
    <li>
      <a href="tc_lecture1.html">Lecture 1</a>
    </li>
    <li>
      <a href="tc_ps1.html">Problem Set 1</a>
    </li>
    <li>
      <a href="tc_lecture2.html">Lecture 2</a>
    </li>
    <li>
      <a href="tc_ps2.html">Problem Set 2</a>
    </li>
    <li>
      <a href="tc_lecture3.html">Lecture 3</a>
    </li>
    <li>
      <a href="tc_ps3.html">Problem Set 3</a>
    </li>
    <li>
      <a href="tc_lecture4.html">Lecture 4</a>
    </li>
    <li>
      <a href="tc_lecture5.html">Lecture 5</a>
    </li>
    <li>
      <a href="more_resources.html">More resources</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Lecture 8: Logistic Regression</h1>

</div>


<p>In <a href="lecture7.html">Lecture 7</a>, we predicted field goal
success using a similar strategy to the one we used in <a
href="lecture4.html">Lecture 4</a> to predict batting averages in 2015
using batting averages from 2014. This strategy amounted to
“binning-and-averaging: we divided the dataset into many small bins,
based on the value of the input or predictor, and then averaged the
outcomes within each bin. In <a href="lecture6.html">Lecture 6</a>, we
used <em>linear regression</em> to take this process to the logical
extreme with infinitessimally small bins in the context of predicting a
continuous response. Today, we will use <em>logistic regression</em> to
take the binning-and-averaging to predicting binary responses to the
same logical extreme. Just as our goal with linear regression was to
predict the average outcome for any given input, our ultimate goal with
logistic regression is to produce a <em>probability</em> forecast for
each input. For instance, in the context of NFL field goals, we would
like to know, say, what the probability is that a kicker successfully
converts a 45-yard field goal attempt.</p>
<div id="logistic-regression-with-glm" class="section level2">
<h2>Logistic Regression with glm()</h2>
<p>Before we get started, we will load the tidyverse and modelr packages
and also load the tbls we saved at the end of <a
href="lecture7.html">Lecture 7</a>.</p>
<pre class="r"><code>&gt; library(tidyverse)
&gt; library(modelr)
&gt; 
&gt; load(&quot;data/nfl_fg.RData&quot;)</code></pre>
<p>We can fit a logistic regression model using the function
<code>glm()</code>. In the code block below, we fit a model of the
success probability as a function of distance. You’ll notice that the
syntax is very similar to the <code>lm()</code> syntax we saw in <a
href="lecture6.html">Lecture 6</a>. The major difference is that we have
to include an argument <code>family = binomial</code>. This tells R that
we are fitting a regression model for <em>binary</em> outcomes.</p>
<pre class="r"><code>&gt; logit_distance &lt;- glm(Success~Distance, family = binomial, data = fg_train)</code></pre>
<p>We can visualize this model fit by (i) creating a grid of distance
values and (ii) plotting the estimated probability of field goal success
at each of the distances in the grid. We can do this using
<code>data_grid()</code> and <code>add_predictions()</code> just like we
did in <a href="lecture6.html">Lecture 6</a>. Notice, however, that in
<code>add_predictions()</code> we have an extra argument
<code>type="response"</code>. In the context of logistic regression,
this argument tells R that we want to return the fitted probabilities
instead of the fitted log-odds.</p>
<pre class="r"><code>&gt; distance_preds &lt;- 
+   fg_train %&gt;%
+   data_grid(Distance) %&gt;%
+   add_predictions(model = logit_distance, type = &quot;response&quot;, var = &quot;Prediction&quot;)
&gt; 
&gt; ggplot(data = distance_preds) + 
+   geom_line(mapping = aes(x = Distance, y = Prediction)) + 
+   ylim(c(0,1))</code></pre>
<p><img src="lecture8_files/figure-html/plot-logit-dist-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Looking at the plot, things look quite reasonable – short field goals
are nearly always made and as the distance increases, the fitted
probability of success goes down in a non-linear fashion. The question
we face now is assessing how well our simple logistic regression model
compares to the “binning-and-averaging” models we built in <a
href="lecture7.html">Lecture 7</a>. In order to assess this, we would
like to append the model predictions to the tbls <code>fg_train</code>
and <code>fg_test</code>, respectively. Rather than using a join, we can
actually do this directly with <code>add_prediction()</code>. The reason
for this is that <code>add_prediction()</code> is able to accept as an
argument the output of <code>lm()</code> or <code>glm()</code>. Since
our “binning-and-averaging” models were not created by either of these
functions we had to use a join in <a href="lecture7.html">Lecture 7</a>.
The codeblock below shows how to add our predictions to
<code>fg_train</code> and <code>fg_test</code> and also computes the
Brier score associated with all of the models we’ve built so far. Notice
that we are saving the predictions in a column called
“phat_dist_logit.”</p>
<pre class="r"><code>&gt; fg_train &lt;-
+   fg_train %&gt;% 
+   add_predictions(model = logit_distance, var = &quot;phat_dist_logit&quot;, type = &quot;response&quot;)
&gt; summarise(fg_train,
+           phat_all = mean( (Success - phat_all)^2),
+           phat_kicker = mean( (Success - phat_kicker)^2),
+           phat_dist_10 = mean( (Success - phat_dist_10)^2),
+           phat_dist_5 = mean( (Success - phat_dist_5)^2),
+           phat_dist_2 = mean( (Success - phat_dist_2)^2),
+           phat_dist_logit = mean( (Success - phat_dist_logit)^2))
# A tibble: 1 × 6
  phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
1    0.140       0.138        0.125       0.123       0.123           0.123</code></pre>
<p>Looking at the Brier scores, we see that our new logistic model fits
the data much better than “phat_all”, the overall average success rate,
and “phat_kicker”, the kicker-specific overall average. Moreover, it
also fits better than the first model we built where we binning the
distances into 10-yard increments. It turns out, however, that our
logistic regression model fits the <strong>training</strong> data
<strong>worse</strong> than the the models which binned the distances
into 5-yard and 2-yard increments. However, to better assess whether the
new logistic regression model is truly better than these two, we have to
look at how well it performs out-of-sample on the
<strong>testing</strong> dataset.</p>
<pre class="r"><code>&gt; fg_test &lt;- 
+   fg_test %&gt;%
+   add_predictions(model = logit_distance, var = &quot;phat_dist_logit&quot;, type = &quot;response&quot;)
&gt; 
&gt; 
&gt; summarise(fg_test,
+           phat_all = mean( (Success - phat_all)^2),
+           phat_kicker = mean( (Success - phat_kicker)^2),
+           phat_dist_10 = mean( (Success - phat_dist_10)^2),
+           phat_dist_5 = mean( (Success - phat_dist_5)^2),
+           phat_dist_2 = mean( (Success - phat_dist_2)^2),
+           phat_dist_logit = mean( (Success - phat_dist_logit)^2))
# A tibble: 1 × 6
  phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit
     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;
1    0.134       0.133        0.120       0.118       0.118           0.117</code></pre>
<p>We see clearly, now, that our logistic regression model has the best
out-of-sample performance. This would indicate that “phat_dist_5” and
“phat_dist_2”, the models formed by binning distances into 5-yard and
2-yard increments and computing the overall success rate within each
bin, over-fit the training data.</p>
</div>
<div id="multiple-regression" class="section level2">
<h2>Multiple Regression</h2>
<p>Up to this point, we have only talked about regression models with a
single predictor (Exception: the end of <a href="ps6.html">Problem Set
6</a> had an example of multiple regression). Though the logistic
regression model we just built out-performs all of the ones we had built
before, it is still pretty limited. After all, for any specific
distance, this model estimates that every kicker has exactly the same
chance of making a field goal. The code below fits a logistic regression
model that accounts for both the kicker and the distance.</p>
<pre class="r"><code>&gt; logit_dist_kicker &lt;- glm(Success ~ Distance + Kicker, family = binomial, data = fg_train)</code></pre>
<p>Before proceeding, notice that the syntax for fitting such a model
with multiple predictors is really similar to the syntax we used above
to fit a simple logistic regression model. In both cases, we used
<code>glm()</code> and specified <code>family = binomial</code> and
<code>data = fg_train</code>. The only difference is on the right hand
side of the <code>~</code> in the <em>formula</em>, the first argument
in <code>glm()</code>. Now, we have
<code>Success ~ Distance + Kicker</code> instead of just
<code>Success ~ Distance</code>. The syntax
<code>Distance + Kicker</code> tells R that we want to include both the
distance and identity of the kicker to predict field goal success.</p>
<p>To visualize the predictions made by this model, we can also use
<code>data_grid()</code>. Since there are so many kickers in our
dataset, we will restrict our attention to just a small handful.</p>
<pre class="r"><code>&gt; dist_kick_grid &lt;-
+   fg_train %&gt;%
+   filter(Kicker %in% c(&quot;Bailey&quot;, &quot;Vinatieri&quot;, &quot;Zuerlein&quot;)) %&gt;%
+   data_grid(Distance, Kicker) %&gt;%
+   add_predictions(model = logit_dist_kicker, var = &quot;Prediction&quot;, type = &quot;response&quot;)
&gt; 
&gt; ggplot(data = dist_kick_grid) +
+   geom_line(mapping = aes(x = Distance, y = Prediction, col = Kicker))</code></pre>
<p><img src="lecture8_files/figure-html/visualize-dist-kicker-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In the call to <code>data_grid()</code> we included all of the
variables that went into the model (<code>Distance</code> and
<code>Kicker</code>). This creates a tbl with every combination of
distance and kicker. Note that if we had not filtered to just three
kickers, the resulting tbl would incredibly long. Just like we did
earlier, in our call to <code>geom_line()</code>, we specified that we
wanted to plot distance on the x-axis and the predicted success
probability on the y-axis. However, we now have an additional aesthetic
<code>col = Kicker</code>. Since our model predicts different
probabilities for different kickers, this additional aesthetic tells
ggplot to use a separate color for each kicker’s probability curve. We
can see that the model predicts Dan Bailey to have consistently higher
chances of converting a field goal successfull than either Greg Zuerlein
or Adam Vinatieri.</p>
<p>Just like we did above, we can also assess the in-sample and
out-of-sample prediction performance by computing the Brier score:</p>
<pre class="r"><code>&gt; fg_train &lt;-
+   fg_train %&gt;%
+   add_predictions(model = logit_dist_kicker, type = &quot;response&quot;, var = &quot;phat_dist_kick_logit&quot;)
&gt; summarise(fg_train,
+           phat_all = mean( (Success - phat_all)^2),
+           phat_kicker = mean( (Success - phat_kicker)^2),
+           phat_dist_10 = mean( (Success - phat_dist_10)^2),
+           phat_dist_5 = mean( (Success - phat_dist_5)^2),
+           phat_dist_2 = mean( (Success - phat_dist_2)^2),
+           phat_dist_logit = mean( (Success - phat_dist_logit)^2),
+           phat_dist_kick_logit= mean( (Success - phat_dist_kick_logit)^2))
# A tibble: 1 × 7
  phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit phat_dist_kick_logit
     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt;
1    0.140       0.138        0.125       0.123       0.123           0.123                0.121</code></pre>
<pre class="r"><code>&gt; fg_test &lt;-
+   fg_test %&gt;%
+   add_predictions(model = logit_dist_kicker, type = &quot;response&quot;, var = &quot;phat_dist_kick_logit&quot;)
&gt; summarise(fg_test,
+           phat_all = mean( (Success - phat_all)^2),
+           phat_kicker = mean( (Success - phat_kicker)^2),
+           phat_dist_10 = mean( (Success - phat_dist_10)^2),
+           phat_dist_5 = mean( (Success - phat_dist_5)^2),
+           phat_dist_2 = mean( (Success - phat_dist_2)^2),
+           phat_dist_logit = mean( (Success - phat_dist_logit)^2),
+           phat_dist_kick_logit= mean( (Success - phat_dist_kick_logit)^2))
# A tibble: 1 × 7
  phat_all phat_kicker phat_dist_10 phat_dist_5 phat_dist_2 phat_dist_logit phat_dist_kick_logit
     &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt;
1    0.134       0.133        0.120       0.118       0.118           0.117                0.118</code></pre>
<p>It turns out that even though accounting for the distance and kicker
resulted in even better in-sample performance (i.e. lower Brier score on
the training data), the out-of-sample performance was worse than the
model that accounted only for the distance.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
